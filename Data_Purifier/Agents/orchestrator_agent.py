# agents/orchestrator_agent.py
# This agent acts as the central control unit for the entire data purification pipeline.
# It orchestrates the execution of specialized agents, manages the flow of data,
# and implements feedback loops for adaptive processing.

import json
import logging
from typing import Dict, List, Optional, Tuple

import pandas as pd
from crewai import Agent # Used for defining the agent's role, goal, and backstory
from data_purifier.utils.cached_chat_openai import CachedChatOpenAI

# Import all specialized agents and their corresponding validator agents
from data_purifier.agents.cleaner_agent import CleanerAgent
from data_purifier.agents.cleaning_validator_agent import CleaningValidatorAgent
from data_purifier.agents.modification_validator_agent import ModificationValidatorAgent
from data_purifier.agents.modifier_agent import DataModifierAgent
from data_purifier.agents.transformation_validator_agent import TransformationValidatorAgent
from data_purifier.agents.transformer_agent import TransformerAgent


class OrchestratorAgent:
    """
    The OrchestratorAgent is the brain of the data purification system.
    It defines the overall pipeline, delegates tasks to other agents,
    and manages the iterative refinement process based on validation feedback.
    """

    def __init__(self, process_recorder_agent, config: Dict):
        """
        Initializes the OrchestratorAgent.

        Args:
            process_recorder_agent: An instance of ProcessRecorderAgent for logging activities.
            config (Dict): Configuration dictionary for the system.
        """
        # Define the CrewAI Agent with its specific role, goal, and backstory
        self.agent = Agent(
            role="Chief Data Strategy Officer",
            goal="To architect, optimize, and govern the end-to-end data purification pipeline, ensuring that all data assets are transformed into highly reliable, analytically-ready, and strategically valuable resources. This involves proactive problem-solving, adaptive strategy formulation based on real-time feedback, and the orchestration of specialized data teams to achieve unparalleled data quality and utility, mirroring the comprehensive oversight of a top-tier data executive.",
            backstory="A visionary leader with extensive experience in data governance, pipeline architecture, and strategic data asset management. This agent possesses a unique ability to foresee data challenges, design resilient and self-optimizing purification workflows, and drive continuous improvement through intelligent feedback mechanisms. Their expertise lies in translating complex data requirements into actionable strategies, ensuring every data point contributes maximally to business intelligence and advanced analytics initiatives.",
            allow_delegation=True, # This agent delegates tasks to other specialized agents
            verbose=True # Enable verbose output for the agent's actions
        )
        
        # Store the ProcessRecorderAgent instance for logging throughout the pipeline
        self.process_recorder_agent = process_recorder_agent

        # Store the global configuration
        self.config = config

        # Initialize the CachedChatOpenAI instance to be used by all agents
        self.llm = CachedChatOpenAI(temperature=self.config.get("llm_temperature", 0.7), model_name=self.config.get("llm_model", "gpt-4"))

        # Initialize all 'employee' agents that the Orchestrator will delegate tasks to
        self.data_cleaner_agent = CleanerAgent(config=self.config, llm=self.llm)
        self.cleaning_validator_agent = CleaningValidatorAgent(config=self.config, llm=self.llm)
        self.data_modifier_agent = DataModifierAgent(config=self.config, llm=self.llm)
        self.modification_validator_agent = ModificationValidatorAgent(config=self.config, llm=self.llm)
        self.data_transformer_agent = TransformerAgent(config=self.config, llm=self.llm)
        self.transformation_validator_agent = TransformationValidatorAgent(config=self.config, llm=self.llm)

        # Logger for internal logging within the orchestrator
        self.logger = logging.getLogger(__name__)
        
        # Attributes to store the pipeline plan and processing instructions received from MetaAnalyzerAgent
        self.pipeline_plan: List[str] = []
        self.processing_instructions: Dict = {}
        
        # Stores learned optimizations from validation feedback for iterative refinement
        self.learned_optimizations: Dict = {}

    def set_processing_instructions(self, meta_analysis_report: Dict):
        """
        Sets the pipeline plan and processing instructions based on the report
        generated by the MetaAnalyzerAgent.

        Args:
            meta_analysis_report (Dict): The comprehensive report from MetaAnalyzerAgent,
                                         containing 'pipeline_plan' and 'suggested_operations'.
        """
        self.pipeline_plan = meta_analysis_report.get("pipeline_plan", [])
        self.processing_instructions = meta_analysis_report.get("suggested_operations", {})
        self.logger.info(f"Orchestrator received pipeline plan: {self.pipeline_plan}")
        self.logger.info(f"Orchestrator received processing instructions: {json.dumps(self.processing_instructions, indent=2)}")

    def orchestrate_data_processing(self, df: pd.DataFrame, meta_output_path: str, report_path: str) -> Tuple[bool, Optional[pd.DataFrame]]:
        """
        Orchestrates the end-to-end data purification pipeline.
        It executes stages (cleaning, modification, transformation) sequentially,
        with validation and retry mechanisms.

        Args:
            df (pd.DataFrame): The initial DataFrame to be processed (from MetaAnalyzerAgent).
            meta_output_path (str): Path to the meta output file (for logging context).
            report_path (str): Base path for saving the final processed dataset and report.

        Returns:
            Tuple[bool, Optional[pd.DataFrame]]: A tuple indicating success (True/False)
                                                 and the final processed DataFrame if successful.
        """
        # Record the initiation of the data purification pipeline
        self.process_recorder_agent.record_task_activity(
            agent_name="OrchestratorAgent",
            task_description=f"Starting data purification pipeline for in-memory dataset",
            status="Initiated",
            details={"meta_output_path": meta_output_path, "report_path": report_path}
        )

        current_df = df # The DataFrame that will be passed through the pipeline stages
        dataset_identifier = "In-memory Dataset" # Identifier for logging purposes

        # Execute the dynamic pipeline plan received from MetaAnalyzerAgent
        for stage in self.pipeline_plan: # Iterate through stages defined in pipeline_plan
            self.logger.info(f"\n--- Executing Pipeline Stage: {stage.upper()} ---")

            if stage == 'cleaning':
                max_cleaning_retries = self.config.get("max_cleaning_retries", 3) # Configurable max retries
                cleaning_attempt = 0
                while cleaning_attempt < max_cleaning_retries:
                    cleaning_attempt += 1
                    self.process_recorder_agent.record_task_activity("OrchestratorAgent", f"Delegating cleaning for {dataset_identifier} to CleanerAgent (Attempt {cleaning_attempt})", "In Progress")
                    try:
                        # Pass the relevant cleaning instructions to the CleanerAgent
                        current_df, cleaning_report = self.data_cleaner_agent.clean_dataset(
                            current_df,
                            self.processing_instructions.get("cleaning_operations", {}),
                            self.learned_optimizations.get("cleaning", {})
                        )
                        self.process_recorder_agent.record_task_activity("OrchestratorAgent", f"Data cleaning completed for {dataset_identifier}. Shape: {current_df.shape}", "Success", cleaning_report, reason="Cleaning stage completed successfully.")
                    except Exception as e:
                        error_message = f"Data cleaning failed for {dataset_identifier}. Error: {str(e)}"
                        self.process_recorder_agent.record_task_activity("OrchestratorAgent", error_message, "Failed")
                        self.logger.error(error_message, exc_info=True)
                        return False, None # Halt pipeline on critical cleaning failure

                    self.process_recorder_agent.record_task_activity("OrchestratorAgent", f"Delegating cleaning validation for {dataset_identifier} to CleaningValidatorAgent", "In Progress")
                    try:
                        # Validate the cleaning results
                        validation_report, validation_success, cleaning_recommendations = self.cleaning_validator_agent.validate_cleaning(
                            current_df,
                            self.processing_instructions.get("cleaning_operations", {}) # Pass original instructions for context
                        )
                        if not validation_success: # If validation fails
                            error_message = f"Cleaning validation failed for {dataset_identifier}. Report: {json.dumps(validation_report, indent=2)}"
                            self.process_recorder_agent.record_task_activity("OrchestratorAgent", error_message, "Failed", validation_report)
                            self.logger.warning(error_message) # Log as warning for potential re-attempt
                            
                            # If recommendations are provided and retries are left, apply them and re-attempt
                            if cleaning_recommendations and cleaning_attempt < max_cleaning_retries:
                                self.logger.info(f"Applying cleaning recommendations and re-attempting cleaning: {cleaning_recommendations}")
                                # Update learned_optimizations with cleaning recommendations
                                self.learned_optimizations.setdefault("cleaning", {}).update(cleaning_recommendations.get("cleaning_operations", {}))
                                # Merge recommendations into the current processing_instructions for the next attempt
                                for col, ops in cleaning_recommendations.get("cleaning_operations", {}).items():
                                    self.processing_instructions.setdefault(col, []).extend(ops)
                                continue # Continue to the next iteration of the while loop (re-attempt cleaning)
                            else:
                                self.logger.error(f"Cleaning validation failed and no more retries or recommendations. Halting pipeline.")
                                return False, None # Halt pipeline if no more retries or recommendations
                        else: # If validation passes
                            self.process_recorder_agent.record_task_activity("OrchestratorAgent", f"Cleaning validation passed for {dataset_identifier}.", "Success", validation_report)
                            break # Exit retry loop if validation passes
                    except Exception as e:
                        error_message = f"Error during cleaning validation for {dataset_identifier}. Halting pipeline. Error: {str(e)}"
                        self.process_recorder_agent.record_task_activity("OrchestratorAgent", error_message, "Failed")
                        self.logger.error(error_message, exc_info=True)
                        return False, None # Halt pipeline on critical validation error
                else: # This else block executes if the while loop completes without a 'break' (i.e., all retries failed)
                    self.logger.error(f"Cleaning stage failed after {max_cleaning_retries} attempts. Halting pipeline.")
                    return False, None

            elif stage == 'modification':
                max_modification_retries = self.config.get("max_modification_retries", 3) # Configurable max retries
                modification_attempt = 0
                while modification_attempt < max_modification_retries:
                    modification_attempt += 1
                    self.process_recorder_agent.record_task_activity("OrchestratorAgent", f"Delegating modification for {dataset_identifier} to DataModifierAgent (Attempt {modification_attempt})", "In Progress")
                    try:
                        # Pass the relevant modification instructions to the DataModifierAgent
                        current_df, modification_report = self.data_modifier_agent.modify_dataset(
                            current_df,
                            self.processing_instructions.get("modification_operations", {}),
                            self.learned_optimizations.get("modification", {})
                        )
                        self.process_recorder_agent.record_task_activity("OrchestratorAgent", f"Data modification completed for {dataset_identifier}. Shape: {current_df.shape}", "Success", modification_report, reason="Modification stage completed successfully.")
                    except Exception as e:
                        error_message = f"Data modification failed for {dataset_identifier}. Error: {str(e)}"
                        self.process_recorder_agent.record_task_activity("OrchestratorAgent", error_message, "Failed")
                        self.logger.error(error_message, exc_info=True)
                        return False, None

                    self.process_recorder_agent.record_task_activity("OrchestratorAgent", f"Delegating modification validation for {dataset_identifier} to ModificationValidatorAgent", "In Progress")
                    try:
                        # Validate the modification results
                        mod_val_report, mod_val_success, modification_recommendations = self.modification_validator_agent.validate_modification(
                            current_df,
                            self.processing_instructions.get("modification_operations", {}) # Pass original instructions for context
                        )
                        if not mod_val_success: # If validation fails
                            error_message = f"Modification validation failed for {dataset_identifier}. Report: {json.dumps(mod_val_report, indent=2)}"
                            self.process_recorder_agent.record_task_activity("OrchestratorAgent", error_message, "Failed", mod_val_report)
                            self.logger.warning(error_message) # Log as warning for potential re-attempt

                            # If recommendations are provided and retries are left, apply them and re-attempt
                            if modification_recommendations and modification_attempt < max_modification_retries:
                                self.logger.info(f"Applying modification recommendations and re-attempting modification: {modification_recommendations}")
                                # Update learned_optimizations with modification recommendations
                                self.learned_optimizations.setdefault("modification", {}).update(modification_recommendations.get("modification_operations", {}))
                                # Merge recommendations into the current processing_instructions for the next attempt
                                for col, ops in modification_recommendations.get("modification_operations", {}).items():
                                    self.processing_instructions.setdefault(col, []).extend(ops)
                                continue # Continue to the next iteration of the while loop (re-attempt modification)
                            else:
                                self.logger.error(f"Modification validation failed and no more retries or recommendations. Halting pipeline.")
                                return False, None # Halt pipeline if no more retries or recommendations
                        else: # If validation passes
                            self.process_recorder_agent.record_task_activity("OrchestratorAgent", f"Modification validation passed for {dataset_identifier}.", "Success", mod_val_report)
                            break # Exit retry loop if validation passes
                    except Exception as e:
                        error_message = f"Error during modification validation for {dataset_identifier}. Halting pipeline. Error: {str(e)}"
                        self.process_recorder_agent.record_task_activity("OrchestratorAgent", error_message, "Failed")
                        self.logger.error(error_message, exc_info=True)
                        return False, None # Halt pipeline on critical validation error
                else: # This else block executes if the while loop completes without a 'break' (i.e., all retries failed)
                    self.logger.error(f"Modification stage failed after {max_modification_retries} attempts. Halting pipeline.")
                    return False, None

            elif stage == 'transformation':
                max_transformation_retries = self.config.get("max_transformation_retries", 3) # Configurable max retries
                transformation_attempt = 0
                while transformation_attempt < max_transformation_retries:
                    transformation_attempt += 1
                    self.process_recorder_agent.record_task_activity("OrchestratorAgent", f"Delegating transformation for {dataset_identifier} to TransformerAgent (Attempt {transformation_attempt})", "In Progress")
                    try:
                        # Pass the relevant transformation instructions to the TransformerAgent
                        current_df, transformation_report = self.data_transformer_agent.transform_data(
                            current_df,
                            self.processing_instructions.get("transformation_operations", {}),
                            self.learned_optimizations.get("transformation", {})
                        )
                        self.process_recorder_agent.record_task_activity("OrchestratorAgent", f"Data transformation completed for {dataset_identifier}. Shape: {current_df.shape}", "Success", transformation_report, reason="Transformation stage completed successfully.")
                    except Exception as e:
                        error_message = f"Data transformation failed for {dataset_identifier}. Error: {str(e)}"
                        self.process_recorder_agent.record_task_activity("OrchestratorAgent", error_message, "Failed")
                        self.logger.error(error_message, exc_info=True)
                        return False, None

                    self.process_recorder_agent.record_task_activity("OrchestratorAgent", f"Delegating transformation validation for {dataset_identifier} to TransformationValidatorAgent", "In Progress")
                    try:
                        # Validate the transformation results
                        trans_val_report, trans_val_success, transformation_recommendations = self.transformation_validator_agent.validate_transformation(
                            current_df,
                            self.processing_instructions.get("transformation_operations", {}) # Pass original instructions for context
                        )
                        if not trans_val_success: # If validation fails
                            error_message = f"Transformation validation failed for {dataset_identifier}. Report: {json.dumps(trans_val_report, indent=2)}"
                            self.process_recorder_agent.record_task_activity("OrchestratorAgent", error_message, "Failed", trans_val_report)
                            self.logger.warning(error_message) # Log as warning for potential re-attempt

                            # If recommendations are provided and retries are left, apply them and re-attempt
                            if transformation_recommendations and transformation_attempt < max_transformation_retries:
                                self.logger.info(f"Applying transformation recommendations and re-attempting transformation: {transformation_recommendations}")
                                # Update learned_optimizations with transformation recommendations
                                self.learned_optimizations.setdefault("transformation", {}).update(transformation_recommendations.get("transformation_operations", {}))
                                # Merge recommendations into the current processing_instructions for the next attempt
                                for col, ops in transformation_recommendations.get("transformation_operations", {}).items():
                                    self.processing_instructions.setdefault(col, []).extend(ops)
                                continue # Continue to the next iteration of the while loop (re-attempt transformation)
                            else:
                                self.logger.error(f"Transformation validation failed and no more retries or recommendations. Halting pipeline.")
                                return False, None # Halt pipeline if no more retries or recommendations
                        else: # If validation passes
                            self.process_recorder_agent.record_task_activity("OrchestratorAgent", f"Transformation validation passed for {dataset_identifier}.", "Success", trans_val_report)
                            break # Exit retry loop if validation passes
                    except Exception as e:
                        error_message = f"Error during transformation validation for {dataset_identifier}. Halting pipeline. Error: {str(e)}"
                        self.process_recorder_agent.record_task_activity("OrchestratorAgent", error_message, "Failed")
                        self.logger.error(error_message, exc_info=True)
                        return False, None # Halt pipeline on critical validation error
                else: # This else block executes if the while loop completes without a 'break' (i.e., all retries failed)
                    error_message = f"Transformation stage failed after {max_transformation_retries} attempts for {dataset_identifier}. Halting pipeline."
                    self.process_recorder_agent.record_task_activity("OrchestratorAgent", error_message, "Failed")
                    self.logger.error(error_message)
                    return False, None

        # --- Final Data Saving ---
        # Save the processed DataFrame to the specified output path.
        self.process_recorder_agent.record_task_activity("OrchestratorAgent", f"Saving processed dataset from {dataset_identifier}", "In Progress")
        try:
            # Determine output format (defaults to CSV)
            output_format = self.config.get("output_file_format", "csv").lower()
            if output_format == "parquet":
                # Ensure the output path has a .parquet extension
                output_path_with_ext = f"{report_path.rsplit('.', 1)[0]}.parquet" if '.' in report_path else f"{report_path}.parquet"
                current_df.to_parquet(output_path_with_ext, index=False)
                self.process_recorder_agent.record_task_activity("OrchestratorAgent", f"Processed dataset saved to {output_path_with_ext} in Parquet format", "Success")
            else:
                # Default to CSV format
                output_path_with_ext = f"{report_path.rsplit('.', 1)[0]}.csv" if '.' in report_path else f"{report_path}.csv"
                current_df.to_csv(output_path_with_ext, index=False)
                self.process_recorder_agent.record_task_activity("OrchestratorAgent", f"Processed dataset saved to {output_path_with_ext} in CSV format", "Success")
        except Exception as e:
            error_message = f"Failed to save processed dataset for {dataset_identifier}. Error: {str(e)}"
            self.process_recorder_agent.record_task_activity("OrchestratorAgent", error_message, "Failed")
            self.logger.error(error_message, exc_info=True)
            return False, None # Halt pipeline on save failure

        # --- Final Reporting ---
        # Generate and save the comprehensive final report of the pipeline run.
        self.process_recorder_agent.record_task_activity("OrchestratorAgent", "Generating final pipeline report.", "In Progress")
        try:
            final_report = self.process_recorder_agent.generate_final_report(report_path)
            self.process_recorder_agent.record_task_activity("OrchestratorAgent", f"Final pipeline report generated and saved to {report_path}.json", "Success", final_report)
        except Exception as e:
            error_message = f"Failed to generate final pipeline report. Error: {str(e)}"
            self.process_recorder_agent.record_task_activity("OrchestratorAgent", error_message, "Failed")
            self.logger.error(error_message, exc_info=True)

        self.process_recorder_agent.record_task_activity("OrchestratorAgent", "Data purification pipeline finished.", "Completed")
        return True, current_df